{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5504c7fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# О Проекте: \n",
    "\n",
    "Запрос сформирован тем, что просмотр фильмов на оригинальном языке - это популярный и действенный метод прокачаться при изучении иностранных языков. Важно выбрать фильм, который подходит студенту по уровню сложности, т.е. студент понимал 50-70 % диалогов. Чтобы выполнить это условие, преподаватель должен посмотреть фильм и решить, какому уровню он соответствует. Однако это требует больших временных затрат.\n",
    "\n",
    "**Задача:**\n",
    "\n",
    "- разработать ML решение для автоматического определения уровня сложности англоязычных фильмов по тексту субтитров. \n",
    "\n",
    "**Данные в распоряжении:**\n",
    "- словари Oxford, в которых слова распределены по уровню сложности\n",
    "- набор файлов-субтитров, рассортированных по каталогам в соответствии с уровнем сложности\n",
    "- excel-файл со список несортированных фильмов и указанием их уровня без текста субтитров\n",
    "\n",
    "**План выполнения задачи:**\n",
    "- изучение и обработка предоставленного материала\n",
    "- обработка текста субтитров и подготовка для машинного обучения\n",
    "- тестирование модели и подбор гиперпараметров\n",
    "\n",
    "**Обоснование выбора модели:**\n",
    "\n",
    "***Алгоритм Doc2Vec***\n",
    "\n",
    "В данной модели векторные представления документов обучаются предсказывать слова в документе, точнее берется вектор документа и объединяется с несколькими векторами слов из него, и модель пытается\n",
    "предсказать следующее слово с учетом контекста. Данная особенность алгоритма позволит нам лучше определить отношение текста субтитров к тому или иному уровню английского языка. Для обучения мы воспользуемся исключительно текстовым содержанием фильмов.\n",
    "Такие параметры как скорость речи или длительность реплики не берутся во внимание в данном случае. Это отдельная часть исследования, которое в нашем случае, опускается. Основной фокус внимания был сделан именно на текст и его анализ.\n",
    "\n",
    "В качестве классификатора мы будем использовать RandomForestClassifier, который менее склонен к переобучению. В связи с малым количеством предоставленных данных воспользуемся более тонкими настройками модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a16cd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import gensim\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import spacy\n",
    "\n",
    "try:\n",
    "    import chardet\n",
    "except:\n",
    "    !pip install chardet\n",
    "    import chardet\n",
    "\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "except:\n",
    "    !pip install pypdf\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "try:\n",
    "    import pysrt\n",
    "except:\n",
    "    !pip install pysrt\n",
    "    import pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8da1824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "SCORES_PATH    = 'English_level/English_scores'\n",
    "SUBTITLES_PATH = 'English_level/English_scores/Subtitles_all'\n",
    "OXFORD_PATH    = 'English_level/Oxford_CEFR_level'\n",
    "\n",
    "ENGLISH_LEVELS = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "RANDOM_STATE = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f98292",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84bdd3",
   "metadata": {},
   "source": [
    "## The Oxford CEFR level\n",
    "\n",
    "В словарях Oxford 3000 и 5000 содержатся наиболее важные слова, которые должен знать каждый, кто учит английский.\n",
    "\n",
    "В нашем случае, поскольку оценка фильма и присвоение ему уровня сложности будет происходить исключительно по тексту субтитров фильмов, часть предоставленных данных в исследовании учитываться не будут. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af22f02",
   "metadata": {},
   "source": [
    "## Файлы с субтитрами\n",
    "\n",
    "Субтитры представлены в виде файлов .srt, где есть номер реплики, время и текст.\n",
    "\n",
    "Файлы рассортированы по папкам, соответствующим уровням сложности языка.\n",
    "\n",
    "Просканируем все файлы построчно:\n",
    "- удалим из текста по возможности тэги, скобки, указания говорящего лица\n",
    "- нетекстовые знаки, \n",
    "- лишние пробелы и переводы строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка: файл .DS_Store другого формата\n",
      "Ошибка: файл .DS_Store другого формата\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>duration</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crown, The S01E10 - Gloriana.en.FORCED</td>\n",
       "      <td>i am delighted to be here in cairo to meet wit...</td>\n",
       "      <td>[3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crown, The S01E04 - Act of God.en</td>\n",
       "      <td>fuel on. fuel on. chocks are in position. swit...</td>\n",
       "      <td>[2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...</td>\n",
       "      <td>good morning, connor. versace is on 1. okay. c...</td>\n",
       "      <td>[2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0             Crown, The S01E10 - Gloriana.en.FORCED   \n",
       "1                  Crown, The S01E04 - Act of God.en   \n",
       "2  Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...   \n",
       "\n",
       "                                             content  \\\n",
       "0  i am delighted to be here in cairo to meet wit...   \n",
       "1  fuel on. fuel on. chocks are in position. swit...   \n",
       "2  good morning, connor. versace is on 1. okay. c...   \n",
       "\n",
       "                                            duration level  \n",
       "0  [3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...    B2  \n",
       "1  [2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...    B2  \n",
       "2  [2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...    B2  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_line(line):\n",
    "    if re.search(r'[A-Za-z]',line): \n",
    "        line = line.lower()\n",
    "        line = re.sub(r'\\n', ' ', line)                            # remove new lines\n",
    "        line = re.sub(r'- ', ' ', line)                            # remove dash\n",
    "        line = re.sub(r'\\<[^\\<]+?\\>', '', line)                    # remove html tags\n",
    "        line = re.sub(r'\\([^\\(]+?\\)', '', line)                    # remove () parenthesis\n",
    "        line = re.sub(r'\\[[^\\[]+?\\]', '', line)                    # remove [] parenthesis\n",
    "        line = re.sub(r'^([\\w#\\s]+\\:)', ' ', line)                 # remove speaker tag\n",
    "        line = re.sub(r'[^[:alnum:][:punct:][:blank:]]',' ', line) # remove all other non-speach shars\n",
    "        line = re.sub(r'\\s\\s+', ' ', line).strip()                 # remove extra spaces\n",
    "    return line\n",
    "\n",
    "\n",
    "# обработка текста построчно\n",
    "def process_text(content):\n",
    "    text = []\n",
    "    duration  = []\n",
    "    for item in content:\n",
    "        if not hasattr(item, 'duration'):\n",
    "            print('no')\n",
    "        if item.duration.ordinal>0:\n",
    "            line = process_line(item.text_without_tags)\n",
    "            text.append(line)\n",
    "            duration.append(item.duration.ordinal/1000)\n",
    "    return ' '.join(text), duration\n",
    "\n",
    "\n",
    "# обработка файлов формата srt\n",
    "def process_srt(dirname, filename):\n",
    "    global count\n",
    "    if not filename.endswith('.srt'):                        # skip non srt files\n",
    "        print('Ошибка: файл', filename, 'другого формата')\n",
    "        return False\n",
    "    fullpath = os.path.join(dirname,filename)\n",
    "    try:\n",
    "        enc = chardet.detect(open(fullpath, \"rb\").read())['encoding']\n",
    "        content = pysrt.open(fullpath, encoding=enc)\n",
    "    except:\n",
    "        print('Ошибка: файл не прочитан', filename)\n",
    "        return False\n",
    "    return process_text(content)                            # clean text and return\n",
    "\n",
    "\n",
    "# movies dataset template\n",
    "movies = pd.DataFrame(columns=['filename', \n",
    "                               'content', \n",
    "                               'duration', \n",
    "                               'level']\n",
    "                     )\n",
    "\n",
    "# recursive walk through dirs\n",
    "for dirname, _, filenames in os.walk(SUBTITLES_PATH):\n",
    "    for filename in filenames:\n",
    "        level  = dirname.split('/')[-1]                   # get level name from dir\n",
    "        result = process_srt(dirname, filename)           # process file\n",
    "        if result:                                        # add movie to dataframe\n",
    "            subs, duration = result\n",
    "            movies.loc[len(movies)] = \\\n",
    "                {'filename' : filename.replace('.srt', ''),\n",
    "                 'content'  : subs,\n",
    "                 'duration' : duration, \n",
    "                 'level'    : level\n",
    "                }\n",
    "\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914eadf-60cf-4412-a009-bb87d29b3804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca37763",
   "metadata": {},
   "source": [
    "## Обработка excel-файла\n",
    "\n",
    "В данном файле содержится список фильмов и уровни сложности.\n",
    " \n",
    "Загрузим файл и изучим его:\n",
    "- удалим дубликаты\n",
    "- исправим несовпадения названий\n",
    "- из указанных уровней оставим более сложный вариант\n",
    "- удалим фильмы, для которых нет информации по уровню сложности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e329ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 241 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Movie   241 non-null    object\n",
      " 1   Level   241 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n",
      "\n",
      "Количество полных дубликатов: 2\n",
      "Количество дубликатов в названии фильмов: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               movie   level\n",
       "id                                          \n",
       "0          10_Cloverfield_lane(2016)      B1\n",
       "1   10_things_I_hate_about_you(1999)      B1\n",
       "2               A_knights_tale(2001)      B2\n",
       "3               A_star_is_born(2018)      B2\n",
       "4                      Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_labels = pd.read_excel(f'{SCORES_PATH}/movies_labels.xlsx', index_col='id')\n",
    "movie_labels.info()\n",
    "movie_labels.columns = ['movie', 'level']\n",
    "print('\\nКоличество полных дубликатов:', movie_labels.duplicated().sum())\n",
    "print('Количество дубликатов в названии фильмов:', movie_labels.movie.duplicated().sum())\n",
    "movie_labels = movie_labels.drop_duplicates()\n",
    "movie_labels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0885dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_50f82\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_50f82_level0_col0\" class=\"col_heading level0 col0\" >movie</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >level</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row0\" class=\"row_heading level0 row0\" >A2</th>\n",
       "      <td id=\"T_50f82_row0_col0\" class=\"data row0 col0\" >32 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row1\" class=\"row_heading level0 row1\" >B1</th>\n",
       "      <td id=\"T_50f82_row1_col0\" class=\"data row1 col0\" >58 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row2\" class=\"row_heading level0 row2\" >B2</th>\n",
       "      <td id=\"T_50f82_row2_col0\" class=\"data row2 col0\" >109 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row3\" class=\"row_heading level0 row3\" >C1</th>\n",
       "      <td id=\"T_50f82_row3_col0\" class=\"data row3 col0\" >40 фильмов</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2ef150e8f0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct some mistakes in movies names\n",
    "movie_labels.movie = movie_labels.movie.str.replace('.srt', '', regex=False)\n",
    "movie_labels.loc[movie_labels.movie == 'Up (2009)', 'movie'] = 'Up(2009)'\n",
    "movie_labels.loc[movie_labels.movie == 'The Grinch', 'movie'] = 'The.Grinch'\n",
    "\n",
    "# удалим из level лишние символы, разделители оставим пробел\n",
    "# из мультиуровней выберем наибольший\n",
    "movie_labels.level = movie_labels.level \\\n",
    "                                 .str.replace(',', '', regex=False) \\\n",
    "                                 .str.replace('+', '', regex=False) \\\n",
    "                                 .str.replace('/', ' ', regex=False) \\\n",
    "                                 .str.split().transform(lambda x: max(x))\n",
    "\n",
    "movie_labels.groupby('level').count().sort_index().style.format({'movie':'{:.0f} фильмов'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b2f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст не найден для фильма The Secret Life of Pets.en\n",
      "Текст не найден для фильма Glass Onion\n",
      "Текст не найден для фильма Matilda(2022)\n",
      "Текст не найден для фильма Bullet train\n",
      "Текст не найден для фильма Thor: love and thunder\n",
      "Текст не найден для фильма Lightyear\n"
     ]
    }
   ],
   "source": [
    "# excel processing\n",
    "for row in movie_labels.itertuples():\n",
    "\n",
    "    n = movies.loc[movies.filename.str.contains(row.movie, regex=False)].shape[0]\n",
    "\n",
    "    if n == 0:\n",
    "        print('Текст не найден для фильма', row.movie)\n",
    "\n",
    "    elif n == 1:\n",
    "        selected_movie_level = movies.loc[\n",
    "            movies.filename.str.contains(row.movie, regex=False), 'level'].values[0]\n",
    "\n",
    "        if selected_movie_level == 'Subtitles':         # replace Subtitles with excel level\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'] = row.level\n",
    "\n",
    "        elif selected_movie_level != row.level:          # replace with max current level or excel\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'\n",
    "             ] = max(selected_movie_level, row.level)\n",
    "\n",
    "    else:\n",
    "        print('Текст есть в датасете', row.movie)\n",
    "\n",
    "\n",
    "movies = movies[movies.level!='Subtitles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db1473-a9a0-4c87-b946-cacbfb624886",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['target'] = movies.level.map({'A1':0, 'A2':1, 'B1':2, 'B2':3, 'C1':4, 'C2':5})\n",
    "movies = movies.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9664d-69d1-4701-b438-0c820a3d5775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2    144\n",
       "B1     55\n",
       "C1     39\n",
       "A2     32\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66906825-2a52-4133-96a1-142e799d9687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>duration</th>\n",
       "      <th>level</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crown, The S01E10 - Gloriana.en.FORCED</td>\n",
       "      <td>i am delighted to be here in cairo to meet wit...</td>\n",
       "      <td>[3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crown, The S01E04 - Act of God.en</td>\n",
       "      <td>fuel on. fuel on. chocks are in position. swit...</td>\n",
       "      <td>[2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...</td>\n",
       "      <td>good morning, connor. versace is on 1. okay. c...</td>\n",
       "      <td>[2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S01E06.1080p.BluRay.AAC5.1.x265-DTG.02.EN</td>\n",
       "      <td>harvey, i don't need a perp-walk or a front-pa...</td>\n",
       "      <td>[2.252, 1.208, 2.919, 3.044, 2.168, 2.668, 1.8...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suits.S02E04.HDTV.x264-ASAP</td>\n",
       "      <td>i want to, uh... taupe. is that...? justice th...</td>\n",
       "      <td>[2.001, 3.036, 1.634, 0.999, 1.032, 2.167, 1.7...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Suits S04E09 EngSub</td>\n",
       "      <td>previously on suits... i want you to decide if...</td>\n",
       "      <td>[1.187, 4.012, 4.838, 1.878, 1.801, 3.386, 2.4...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Suits.S03E09.480p.HDTV.x264-mSD</td>\n",
       "      <td>previously on suits... i'm bonding with your f...</td>\n",
       "      <td>[1.454, 1.464, 1.692, 1.13, 1.3, 1.851, 2.786,...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Suits S04E10 EngSub</td>\n",
       "      <td>previously on suits... sheila amanda sazs, wil...</td>\n",
       "      <td>[1.205, 1.495, 1.177, 2.345, 2.478, 1.111, 2.7...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Downton Abbey - S01E03 - Episode 3.eng.SDH</td>\n",
       "      <td>there you are, mr. bates. it's in. came this m...</td>\n",
       "      <td>[2.708, 3.118, 1.832, 1.206, 2.287, 2.366, 1.7...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Suits S04E13 EngSub</td>\n",
       "      <td>and you got your name on the wall. now i want ...</td>\n",
       "      <td>[1.361, 1.641, 1.0, 1.143, 1.646, 1.444, 1.177...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  \\\n",
       "0               Crown, The S01E10 - Gloriana.en.FORCED   \n",
       "1                    Crown, The S01E04 - Act of God.en   \n",
       "2    Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...   \n",
       "3      Suits.S01E06.1080p.BluRay.AAC5.1.x265-DTG.02.EN   \n",
       "4                          Suits.S02E04.HDTV.x264-ASAP   \n",
       "..                                                 ...   \n",
       "273                                Suits S04E09 EngSub   \n",
       "274                    Suits.S03E09.480p.HDTV.x264-mSD   \n",
       "275                                Suits S04E10 EngSub   \n",
       "276         Downton Abbey - S01E03 - Episode 3.eng.SDH   \n",
       "277                                Suits S04E13 EngSub   \n",
       "\n",
       "                                               content  \\\n",
       "0    i am delighted to be here in cairo to meet wit...   \n",
       "1    fuel on. fuel on. chocks are in position. swit...   \n",
       "2    good morning, connor. versace is on 1. okay. c...   \n",
       "3    harvey, i don't need a perp-walk or a front-pa...   \n",
       "4    i want to, uh... taupe. is that...? justice th...   \n",
       "..                                                 ...   \n",
       "273  previously on suits... i want you to decide if...   \n",
       "274  previously on suits... i'm bonding with your f...   \n",
       "275  previously on suits... sheila amanda sazs, wil...   \n",
       "276  there you are, mr. bates. it's in. came this m...   \n",
       "277  and you got your name on the wall. now i want ...   \n",
       "\n",
       "                                              duration level  target  \n",
       "0    [3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...    B2       3  \n",
       "1    [2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...    B2       3  \n",
       "2    [2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...    B2       3  \n",
       "3    [2.252, 1.208, 2.919, 3.044, 2.168, 2.668, 1.8...    B2       3  \n",
       "4    [2.001, 3.036, 1.634, 0.999, 1.032, 2.167, 1.7...    B2       3  \n",
       "..                                                 ...   ...     ...  \n",
       "273  [1.187, 4.012, 4.838, 1.878, 1.801, 3.386, 2.4...    C1       4  \n",
       "274  [1.454, 1.464, 1.692, 1.13, 1.3, 1.851, 2.786,...    C1       4  \n",
       "275  [1.205, 1.495, 1.177, 2.345, 2.478, 1.111, 2.7...    C1       4  \n",
       "276  [2.708, 3.118, 1.832, 1.206, 2.287, 2.366, 1.7...    C1       4  \n",
       "277  [1.361, 1.641, 1.0, 1.143, 1.646, 1.444, 1.177...    C1       4  \n",
       "\n",
       "[270 rows x 5 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b829f3d-846f-44a8-8c61-14f7724a8c30",
   "metadata": {},
   "source": [
    "Очевидным является дисбаланс классов, некоторые и вовсе не представлены - 'A1' и 'C2'. Это второй момент, который стоит иметь в виду, помимо очень скромного по объемам датасета для обучения и тестирования модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf21de3-41af-42f3-a232-2188fd3184cc",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b67cb-b8b1-47b0-be9a-978628c1c727",
   "metadata": {},
   "source": [
    "Настройка моделей обучения и оценки Doc2Vec\n",
    "\n",
    "\n",
    "Векторное представление слов (англ. word embedding) — общее название для различных подходов к моделированию языка и обучению представлений в обработке естественного языка, направленных на сопоставление словам из некоторого словаря векторов небольшой размерности. \n",
    "\n",
    "Данный алгоритм сначала создает словарь, а затем вычисляет векторное представление слов. Векторное представление основывается на контекстной близости: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, имеющие схожий смысл), в векторном представлении имеют высокое косинусное сходство. \n",
    "В отличие от мешка слов, матрица эмбедингов не разреженная +  учитывается контекст слова. Во время обучения мы будем использовать не слишком много эпох - чтобы в векторном пространстве слова далеко не разошлись - из-за этого может быть риск \"плохого\" представления слов для нас. Если бы выборка была значимо больше, то мы увеличили бы количетво эпох тренировки. \n",
    "Поэтому нам не нужно специально сопоставлять слова со словарем Oxford. К примеру, есть слово age как существительное, которое относится к уровню знаний A1. Но то же слово как глагол - это уже другой уровень, выше. \n",
    "\n",
    "Соответственно, данный факт необходимо учитывать при обучении. С данной особенностью языка отлично справляется модель. Стоит отметить, что классы дисбалансированы + некоторые имеют расхождение в трактовках \"классов\" - когда A2/A2+, B1. Вспомним \"золотое правило машинного обучения\" -  garbage in - garbage out. Т.е. для потенциального улучшения качества модели в будущем нам потребуется большее количество данных с качественной разметкой (экспертизой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15baef26-d7cd-4167-8a58-f2f1e2c8cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove stop words from word list\n",
    "def remove_stop_words(sample_words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sample_words = [x for x in sample_words if not x in stop_words]\n",
    "    return sample_words\n",
    "\n",
    "##remove special characters from word list\n",
    "def remove_special_char(sample_words):\n",
    "    special_char = set(punctuation) \n",
    "    sample_words = [x for x in sample_words if not x in special_char]    \n",
    "    return sample_words\n",
    "\n",
    "##lemmatize words in word list\n",
    "def lemmatizer(sample_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sample_words = [lemmatizer.lemmatize(x) for x in sample_words]\n",
    "    return sample_words\n",
    "\n",
    "##all words in lower case\n",
    "def lower_case(sample_words):\n",
    "    sample_words = [x.lower() for x in sample_words]\n",
    "    return sample_words\n",
    "\n",
    "##normalize a word list (if document already tokenized)\n",
    "def normalize_word_list(sample_words,\n",
    "                        lowercase=True,\n",
    "                        stopwords=True,\n",
    "                        specialchar=True,\n",
    "                        lemmatize=True):\n",
    "    if lowercase:\n",
    "        sample_words = lower_case(sample_words)\n",
    "    if stopwords:\n",
    "        sample_words = remove_stop_words(sample_words)\n",
    "    if specialchar:\n",
    "        sample_words = remove_special_char(sample_words)\n",
    "    if lemmatize:\n",
    "        sample_words = lemmatizer(sample_words)\n",
    "    sample_words = ' '.join(sample_words)\n",
    "    return sample_words\n",
    "\n",
    "##normalize a list of sentences\n",
    "def normalize_sent_list(sample_sents,\n",
    "                        lowercase=True,\n",
    "                        stopwords=True,\n",
    "                        specialchar=True,\n",
    "                        lemmatize=True):    \n",
    "    print(\"Pre-processing text ...\")\n",
    "    sent_list = sample_sents\n",
    "    for i in range(len(sample_sents)):\n",
    "        sent_list[i] = re.findall(r\"[\\w']+|[.,!?;]\", sent_list[i])\n",
    "        sent_list[i] = normalize_word_list(sent_list[i],\n",
    "                            lowercase=True,\n",
    "                            stopwords=True,\n",
    "                            specialchar=True,\n",
    "                            lemmatize=True)\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71a2a5-f8fd-4f89-8e4c-bb3c4e4b5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data_file):\n",
    "    ## read training data as a pandas dataframe\n",
    "    orig_df = data_file\n",
    "    ## text preprocessing\n",
    "    text = pd.Series.tolist((orig_df['content']))\n",
    "    x_target = pd.Series.tolist((orig_df['target']))\n",
    "    x_level = pd.Series.tolist((orig_df['level']))\n",
    "    \n",
    "    text = normalize_sent_list(text,\n",
    "                        \t\tlowercase=True,\n",
    "                        \t\tstopwords=True,\n",
    "                        \t\tspecialchar=True,\n",
    "                        \t\tlemmatize=False)\n",
    "    ## preparing preprocessed text\n",
    "    text_df = pd.DataFrame(text, columns=[\"content\"])\n",
    "    text_df['level'] = x_level\n",
    "    text_df['target'] = x_target\n",
    "    print(text_df)\n",
    "    return text_df\n",
    "\n",
    "def Doc2VecModel(text_df, no_epochs, val_split_ratio):\n",
    "    ## splitting dataframe into training and validation frames\n",
    "    train_df, val_df = train_test_split(text_df, test_size=val_split_ratio, random_state=RANDOM_STATE)\t\n",
    "    ## creating tagged documents\n",
    "    train_tagged = train_df.apply(\n",
    "        lambda r: TaggedDocument(words=r['content'].split(), tags=str(r.target)), axis=1)\n",
    "    val_tagged = val_df.apply(\n",
    "        lambda r: TaggedDocument(words=r['content'].split(), tags=str(r.target)), axis=1)\n",
    "    ## building a distributed bag of words model \n",
    "    cores = multiprocessing.cpu_count()\n",
    "    print(\"Building the Doc2Vec model vocab...\")\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=2000, negative=5, min_count=3, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    ## training the model\n",
    "    print(\"Training the Doc2Vec model for\", no_epochs, \"number of epochs\" )\n",
    "    for epoch in range(no_epochs):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), \n",
    "                total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    ## preparing document vectors for learning\n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_val, X_val = vec_for_learning(model_dbow, val_tagged)\n",
    "    ## training RandomForestClassifier model\n",
    "    print(\"Training RandomForestClassifier model...\")\n",
    "    rfc=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3],\n",
    "    'min_samples_split': [5, 8, 10],\n",
    "    'n_estimators': [100, 150],\n",
    "    'criterion': ['entropy']\n",
    "    }\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, scoring = 'f1', cv=5)\n",
    "    CV_rfc.fit(X_train, y_train)\n",
    "    print(CV_rfc.best_params_)\n",
    "    b_rfc = CV_rfc.best_estimator_\n",
    "    ## making predictions on the training set\n",
    "    print(\"Prediction numbers:\")\n",
    "    train_binary = b_rfc.predict(X_train)\n",
    "    print('Accuracy on the training set : %s' % accuracy_score(y_train, train_binary))\n",
    "    print('F1 score on the training set : {}'.format(f1_score(y_train, train_binary, average='weighted')))\n",
    "    ## making predictions on the validation set\n",
    "    val_binary = b_rfc.predict(X_val)\n",
    "    print('Accuracy on the validation set : %s' % accuracy_score(y_val, val_binary))\n",
    "    print('F1 score on the validation set : {}'.format(f1_score(y_val, val_binary, average='weighted')))\n",
    "    return model_dbow, b_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aca8101c-bacd-4c44-80e4-d1cb1d2331b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text ...\n",
      "                                               content level  target\n",
      "0    delighted cairo meet colonel nasser continue d...    B2       3\n",
      "1    fuel fuel chock position switch sure sir got m...    B2       3\n",
      "2    good morning connor versace 1 okay clear good ...    B2       3\n",
      "3    harvey need perp walk front page headline want...    B2       3\n",
      "4    want uh taupe justice thomas knew louis ah poi...    B2       3\n",
      "..                                                 ...   ...     ...\n",
      "265  previously suit want decide love hate want com...    C1       4\n",
      "266  previously suit i'm bonding father speaking ta...    C1       4\n",
      "267  previously suit sheila amanda sazs marry yes i...    C1       4\n",
      "268  mr bates came morning said would quite thing h...    C1       4\n",
      "269  got name wall want respect come meaning want o...    C1       4\n",
      "\n",
      "[270 rows x 3 columns]\n",
      "Building the Doc2Vec model vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 229/229 [00:00<00:00, 553280.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Doc2Vec model for 3 number of epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 229/229 [00:00<00:00, 2325655.24it/s]\n",
      "100%|████████████████████████████████████| 229/229 [00:00<00:00, 2395250.91it/s]\n",
      "100%|█████████████████████████████████████| 229/229 [00:00<00:00, 888525.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestClassifier model...\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Prediction numbers:\n",
      "Accuracy on the training set : 1.0\n",
      "F1 score on the training set : 1.0\n",
      "Accuracy on the validation set : 0.6585365853658537\n",
      "F1 score on the validation set : 0.5982632967284168\n"
     ]
    }
   ],
   "source": [
    "no_epochs = 3\n",
    "\n",
    "val_split_ratio = 0.15\n",
    "\n",
    "## preparing training data\n",
    "text_df = prepare_training_data(movies)\n",
    "\n",
    "## building the document vector model\n",
    "model_dbow, classifier = Doc2VecModel(text_df, no_epochs,  val_split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8609b-9b84-4589-bb63-5196bb931b62",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228dd6c8-cb09-4c93-9dcf-c427f1021b13",
   "metadata": {},
   "source": [
    "Как итог, мы получили в рамках ограниченных ресурсов, данных и неполноценной разметки, максимально возможный вариант эффективности модели. \n",
    "Так, мы получили на валидационной выборке результат F1 score приблизительно 60%, что можно считать более чем удовлетворительным при многоклассовой и несбалансированной выборке со спорной разметкой данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
