{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82de4061",
   "metadata": {},
   "source": [
    "<div style=\"border:solid Green 2px; padding: 40px\">\n",
    "    <b> –ü—Ä–∏–≤–µ—Ç, –†–∞–∏—Å–∞!</b>üå∏<br/>\n",
    "    <p>–ú–µ–Ω—è –∑–æ–≤—É—Ç –°–æ—Ñ–∏—è, —è –±—É–¥—É —Ä–µ–≤—å—é–µ—Ä–æ–º —Ç–≤–æ–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. –ü—Ä–µ–¥–ª–∞–≥–∞—é –æ–±—Ä–∞—â–∞—Ç—å—Å—è –Ω–∞ \"—Ç—ã\", –Ω–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ—É–¥–æ–±–Ω–æ–æ - –¥–∞–π –∑–Ω–∞—Ç—å, –∏ –º—ã –ø–µ—Ä–µ–π–¥–µ–º –Ω–∞ \"–≤—ã\".</p>\n",
    "    <p>–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –º–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ —Ç–≤–æ–µ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å–∫—Ä–∞—à–µ–Ω—ã —Ä–∞–∑–Ω—ã–º–∏ —Ü–≤–µ—Ç–∞–º–∏:</p>\n",
    "    <br/>\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> ‚úÖ**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "        <p>–¢–∞–∫–æ–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —à–∞–≥–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏ —è —Ö–æ—á—É —Ç–µ–±—è –ø–æ—Ö–≤–∞–ª–∏—Ç—å –∑–∞ –æ—Ç–ª–∏—á–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é —Ä–∞–±–æ—Ç—É. –ù–∞ —Ä–µ—à–µ–Ω–∏—è, –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ —ç—Ç–∏–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º, –º–æ–∂–Ω–æ –æ–ø–∏—Ä–∞—Ç—å—Å—è –≤ –±—É–¥—É—â–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö. </p>\n",
    "</div>\n",
    "    <br/>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>üí°**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "    <p> –¢–∞–∫–æ–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —è –æ—Å—Ç–∞–≤–ª—è—é –≤ —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —à–∞–≥–µ —Å—Ç–∞–Ω–µ—Ç –µ—â–µ –ª—É—á—à–µ, –µ—Å–ª–∏ –≤–Ω–µ—Å—Ç–∏ –Ω–µ–±–æ–ª—å—à–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏–≤—ã. –¢—ã –º–æ–∂–µ—à—å —É—á–µ—Å—Ç—å —ç—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –±—É–¥—É—â–∏—Ö –∑–∞–¥–∞–Ω–∏–π –∏–ª–∏ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–µ–∫—Ç —Å–µ–π—á–∞—Å (–æ–¥–Ω–∞–∫–æ —ç—Ç–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ). </p>\n",
    "</div>\n",
    "    <br/>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b> ‚ùå**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b> \n",
    "    <p> –¢–∞–∫–æ–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ—Å—Ç–∞–≤–ª–µ–Ω –≤ —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —à–∞–≥–µ —Ç—Ä–µ–±—É–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏ –∏ –≤–Ω–µ—Å–µ–Ω–∏—è –ø—Ä–∞–≤–æ–∫. –ü—Ä–æ–µ–∫—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–Ω—è—Ç –ø–æ–∫–∞ –µ—Å—Ç—å, —Ç–æ —á—Ç–æ –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å. </p>\n",
    "</div>\n",
    "      <p> –ú–æ—è —Ü–µ–ª—å ‚Äî –ø–æ–º–æ—á—å —Ç–µ–±–µ –Ω–∞–π—Ç–∏ –≤–µ—â–∏, –Ω–∞–¥ –∫–æ—Ç–æ—Ä—ã–º–∏ –º–æ–∂–Ω–æ –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å —Ç–≤–æ–π –ø—Ä–æ–µ–∫—Ç.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504c7fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# –û –ü—Ä–æ–µ–∫—Ç–µ: \n",
    "\n",
    "–ó–∞–ø—Ä–æ—Å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω —Ç–µ–º, —á—Ç–æ –ø—Ä–æ—Å–º–æ—Ç—Ä —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —è–∑—ã–∫–µ - —ç—Ç–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∏ –¥–µ–π—Å—Ç–≤–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–∫–∞—á–∞—Ç—å—Å—è –ø—Ä–∏ –∏–∑—É—á–µ–Ω–∏–∏ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –í–∞–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ñ–∏–ª—å–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥—Ö–æ–¥–∏—Ç —Å—Ç—É–¥–µ–Ω—Ç—É –ø–æ —É—Ä–æ–≤–Ω—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Ç.–µ. —Å—Ç—É–¥–µ–Ω—Ç –ø–æ–Ω–∏–º–∞–ª 50-70 % –¥–∏–∞–ª–æ–≥–æ–≤. –ß—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç—Ç–æ —É—Å–ª–æ–≤–∏–µ, –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –¥–æ–ª–∂–µ–Ω –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ñ–∏–ª—å–º –∏ —Ä–µ—à–∏—Ç—å, –∫–∞–∫–æ–º—É —É—Ä–æ–≤–Ω—é –æ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç. –û–¥–Ω–∞–∫–æ —ç—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç.\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:**\n",
    "\n",
    "- —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å ML —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É —Å—É–±—Ç–∏—Ç—Ä–æ–≤. \n",
    "\n",
    "**–î–∞–Ω–Ω—ã–µ –≤ —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏:**\n",
    "- —Å–ª–æ–≤–∞—Ä–∏ Oxford, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ–≤–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ —É—Ä–æ–≤–Ω—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
    "- –Ω–∞–±–æ—Ä —Ñ–∞–π–ª–æ–≤-—Å—É–±—Ç–∏—Ç—Ä–æ–≤, —Ä–∞—Å—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–∞–ª–æ–≥–∞–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —É—Ä–æ–≤–Ω–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
    "- excel-—Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–æ–∫ –Ω–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤ –∏ —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Ö —É—Ä–æ–≤–Ω—è –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤\n",
    "\n",
    "**–ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏:**\n",
    "- –∏–∑—É—á–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞\n",
    "- –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "- —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "\n",
    "**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏:**\n",
    "\n",
    "***–ê–ª–≥–æ—Ä–∏—Ç–º Doc2Vec***\n",
    "\n",
    "–í –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–∞—é—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, —Ç–æ—á–Ω–µ–µ –±–µ—Ä–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Å–ª–æ–≤ –∏–∑ –Ω–µ–≥–æ, –∏ –º–æ–¥–µ–ª—å –ø—ã—Ç–∞–µ—Ç—Å—è\n",
    "–ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –î–∞–Ω–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –ª—É—á—à–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –∫ —Ç–æ–º—É –∏–ª–∏ –∏–Ω–æ–º—É —É—Ä–æ–≤–Ω—é –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º —Ñ–∏–ª—å–º–æ–≤.\n",
    "–¢–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏ –∏–ª–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ–ø–ª–∏–∫–∏ –Ω–µ –±–µ—Ä—É—Ç—Å—è –≤–æ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ. –≠—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ, –æ–ø—É—Å–∫–∞–µ—Ç—Å—è. –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ–∫—É—Å –≤–Ω–∏–º–∞–Ω–∏—è –±—ã–ª —Å–¥–µ–ª–∞–Ω –∏–º–µ–Ω–Ω–æ –Ω–∞ —Ç–µ–∫—Å—Ç –∏ –µ–≥–æ –∞–Ω–∞–ª–∏–∑.\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RandomForestClassifier, –∫–æ—Ç–æ—Ä—ã–π –º–µ–Ω–µ–µ —Å–∫–ª–æ–Ω–µ–Ω –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é. –í —Å–≤—è–∑–∏ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –º–æ–¥–µ–ª–∏. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46e48d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ‚úÖ**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "        <p>–û—Ç–ª–∏—á–Ω–æ, —á—Ç–æ –≤—Å—Ç–∞–≤–ª—è–µ—à—å –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –µ–≥–æ –ø–ª–∞–Ω. –°—Ä–∞–∑—É –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –Ω–∞ –Ω—É–∂–Ω—ã–π –ª–∞–¥, –ø–æ–Ω—è—Ç–Ω–æ —á—Ç–æ –∂–¥–∞—Ç—å –æ—Ç –ø—Ä–æ–µ–∫—Ç–∞.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a16cd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import gensim\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import spacy\n",
    "\n",
    "try:\n",
    "    import chardet\n",
    "except:\n",
    "    !pip install chardet\n",
    "    import chardet\n",
    "\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "except:\n",
    "    !pip install pypdf\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "try:\n",
    "    import pysrt\n",
    "except:\n",
    "    !pip install pysrt\n",
    "    import pysrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082803c1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ‚úÖ**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "        <p>–û—Ç–ª–∏—á–Ω–æ, —á—Ç–æ –∏–º–ø–æ—Ä—Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–±—Ä–∞–Ω—ã –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–æ–µ–∫—Ç–∞: —ç—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∏–ª–µ–≤—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º (PEP 8), –∞ —Ç–∞–∫–∂–µ —ç—Ç–æ —É–¥–æ–±–Ω–æ –≤ —Ä–∞–±–æ—Ç–µ (—Å—Ä–∞–∑—É –≤–∏–¥–Ω—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ —á—Ç–æ –Ω–∞–¥–æ –¥–æ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ —Å–ª—É—á–∞–µ —á–µ–≥–æ). </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a549d37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>üí°**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "    <p> –ß—Ç–æ–±—ã –∫–æ–¥ —Å–º–æ—Ç—Ä–µ–ª—Å—è —á–∏—â–µ –∏ –±–æ–ª–µ–µ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏–º–ø–æ—Ä—Ç –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É, –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–º–ø–æ—Ä—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –æ–¥–Ω–æ–≥–æ –º–æ–¥—É–ª—è. –í–æ—Ç –ø—Ä–∏–º–µ—Ä: </p>\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "\n",
    "# –¥—Ä—É–≥–∞—è —Ñ–æ—Ä–º–∞ –∑–∞–ø–∏—Å–∏\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    "    GridSearchCV\n",
    "    cross_validate\n",
    ")\n",
    "```\n",
    " –ß—Ç–æ–±—ã –±—ã–ª–æ —É–¥–æ–±–Ω–µ–µ —Ä–∞–∑–±–∏—Ä–∞—Ç—å—Å—è –≤ –∏–º–ø–æ—Ä—Ç–∞—Ö —Ç–∞–∫–∂–µ —Å–æ–≤–µ—Ç—É—é —Ä–∞–∑–º–µ—â–∞—Ç—å –∏–º–ø–æ—Ä—Ç—ã –∏–∑ –æ–¥–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥—Ä—É–≥ –ø–æ–¥ –¥—Ä—É–≥–æ–º.    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b3d5a",
   "metadata": {
    "id": "9c-bE6Nl88cP"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>üí°**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "    <p> –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ, –∫–æ–≥–¥–∞ —Ç—ã –≤—ã–≥–ª–∞–¥—ã–≤–∞–µ—à—å –µ–≥–æ –Ω–∞ github —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–ª–æ—Ö–æ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π. –û–±—ã—á–Ω–æ, –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –≤ —Ñ–∞–π–ª–µ <a href=\"https://dvmn.org/encyclopedia/pip/pip_requirements_txt/\">requirements.txt</a>, –æ—Ç—Ç—É–¥–∞ –∏—Ö –∏ –≤–æ–∑—å–º–µ—Ç —Ç–æ—Ç, –∫—Ç–æ –∑–∞—Ö–æ—á–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ç–≤–æ–π –∫–æ–¥. </p>\n",
    "</div>      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8da1824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "SCORES_PATH    = 'English_level/English_scores'\n",
    "SUBTITLES_PATH = 'English_level/English_scores/Subtitles_all'\n",
    "OXFORD_PATH    = 'English_level/Oxford_CEFR_level'\n",
    "\n",
    "ENGLISH_LEVELS = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "RANDOM_STATE = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ba22b",
   "metadata": {
    "id": "9c-bE6Nl88cP"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>üí°**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "    <p> –õ—É—á—à–µ –≤—ã–Ω–µ—Å—Ç–∏ <code>warnings.filterwarnings(\"ignore\")</code> –≤ —è—á–µ–π–∫—É —Å –∏–º–ø–æ—Ä—Ç–∞–º–∏, —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –ø—Ä–æ—Å—Ç–æ —Ç–∞–∫ –ø—Ä–∏–Ω—è—Ç–æ. </p>\n",
    "</div>      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f98292",
   "metadata": {},
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84bdd3",
   "metadata": {},
   "source": [
    "## The Oxford CEFR level\n",
    "\n",
    "–í —Å–ª–æ–≤–∞—Ä—è—Ö Oxford 3000 –∏ 5000 —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–µ–Ω –∑–Ω–∞—Ç—å –∫–∞–∂–¥—ã–π, –∫—Ç–æ —É—á–∏—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.\n",
    "\n",
    "–í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ, –ø–æ—Å–∫–æ–ª—å–∫—É –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–ª—å–º–∞ –∏ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –µ–º—É —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ —Ç–µ–∫—Å—Ç—É —Å—É–±—Ç–∏—Ç—Ä–æ–≤ —Ñ–∏–ª—å–º–æ–≤, —á–∞—Å—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –Ω–µ –±—É–¥—É—Ç. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af22f02",
   "metadata": {},
   "source": [
    "## –§–∞–π–ª—ã —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏\n",
    "\n",
    "–°—É–±—Ç–∏—Ç—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–∏–¥–µ —Ñ–∞–π–ª–æ–≤ .srt, –≥–¥–µ –µ—Å—Ç—å –Ω–æ–º–µ—Ä —Ä–µ–ø–ª–∏–∫–∏, –≤—Ä–µ–º—è –∏ —Ç–µ–∫—Å—Ç.\n",
    "\n",
    "–§–∞–π–ª—ã —Ä–∞—Å—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –ø–∞–ø–∫–∞–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —É—Ä–æ–≤–Ω—è–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–∞.\n",
    "\n",
    "–ü—Ä–æ—Å–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –ø–æ—Å—Ç—Ä–æ—á–Ω–æ:\n",
    "- —É–¥–∞–ª–∏–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç—ç–≥–∏, —Å–∫–æ–±–∫–∏, —É–∫–∞–∑–∞–Ω–∏—è –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –ª–∏—Ü–∞\n",
    "- –Ω–µ—Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–Ω–∞–∫–∏, \n",
    "- –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—à–∏–±–∫–∞: —Ñ–∞–π–ª .DS_Store –¥—Ä—É–≥–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞\n",
      "–û—à–∏–±–∫–∞: —Ñ–∞–π–ª .DS_Store –¥—Ä—É–≥–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>duration</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crown, The S01E10 - Gloriana.en.FORCED</td>\n",
       "      <td>i am delighted to be here in cairo to meet wit...</td>\n",
       "      <td>[3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crown, The S01E04 - Act of God.en</td>\n",
       "      <td>fuel on. fuel on. chocks are in position. swit...</td>\n",
       "      <td>[2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...</td>\n",
       "      <td>good morning, connor. versace is on 1. okay. c...</td>\n",
       "      <td>[2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0             Crown, The S01E10 - Gloriana.en.FORCED   \n",
       "1                  Crown, The S01E04 - Act of God.en   \n",
       "2  Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...   \n",
       "\n",
       "                                             content  \\\n",
       "0  i am delighted to be here in cairo to meet wit...   \n",
       "1  fuel on. fuel on. chocks are in position. swit...   \n",
       "2  good morning, connor. versace is on 1. okay. c...   \n",
       "\n",
       "                                            duration level  \n",
       "0  [3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...    B2  \n",
       "1  [2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...    B2  \n",
       "2  [2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...    B2  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_line(line):\n",
    "    if re.search(r'[A-Za-z]',line): \n",
    "        line = line.lower()\n",
    "        line = re.sub(r'\\n', ' ', line)                            # remove new lines\n",
    "        line = re.sub(r'- ', ' ', line)                            # remove dash\n",
    "        line = re.sub(r'\\<[^\\<]+?\\>', '', line)                    # remove html tags\n",
    "        line = re.sub(r'\\([^\\(]+?\\)', '', line)                    # remove () parenthesis\n",
    "        line = re.sub(r'\\[[^\\[]+?\\]', '', line)                    # remove [] parenthesis\n",
    "        line = re.sub(r'^([\\w#\\s]+\\:)', ' ', line)                 # remove speaker tag\n",
    "        line = re.sub(r'[^[:alnum:][:punct:][:blank:]]',' ', line) # remove all other non-speach shars\n",
    "        line = re.sub(r'\\s\\s+', ' ', line).strip()                 # remove extra spaces\n",
    "    return line\n",
    "\n",
    "\n",
    "# –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ\n",
    "def process_text(content):\n",
    "    text = []\n",
    "    duration  = []\n",
    "    for item in content:\n",
    "        if not hasattr(item, 'duration'):\n",
    "            print('no')\n",
    "        if item.duration.ordinal>0:\n",
    "            line = process_line(item.text_without_tags)\n",
    "            text.append(line)\n",
    "            duration.append(item.duration.ordinal/1000)\n",
    "    return ' '.join(text), duration\n",
    "\n",
    "\n",
    "# –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Ñ–æ—Ä–º–∞—Ç–∞ srt\n",
    "def process_srt(dirname, filename):\n",
    "    global count\n",
    "    if not filename.endswith('.srt'):                        # skip non srt files\n",
    "        print('–û—à–∏–±–∫–∞: —Ñ–∞–π–ª', filename, '–¥—Ä—É–≥–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞')\n",
    "        return False\n",
    "    fullpath = os.path.join(dirname,filename)\n",
    "    try:\n",
    "        enc = chardet.detect(open(fullpath, \"rb\").read())['encoding']\n",
    "        content = pysrt.open(fullpath, encoding=enc)\n",
    "    except:\n",
    "        print('–û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –ø—Ä–æ—á–∏—Ç–∞–Ω', filename)\n",
    "        return False\n",
    "    return process_text(content)                            # clean text and return\n",
    "\n",
    "\n",
    "# movies dataset template\n",
    "movies = pd.DataFrame(columns=['filename', \n",
    "                               'content', \n",
    "                               'duration', \n",
    "                               'level']\n",
    "                     )\n",
    "\n",
    "# recursive walk through dirs\n",
    "for dirname, _, filenames in os.walk(SUBTITLES_PATH):\n",
    "    for filename in filenames:\n",
    "        level  = dirname.split('/')[-1]                   # get level name from dir\n",
    "        result = process_srt(dirname, filename)           # process file\n",
    "        if result:                                        # add movie to dataframe\n",
    "            subs, duration = result\n",
    "            movies.loc[len(movies)] = \\\n",
    "                {'filename' : filename.replace('.srt', ''),\n",
    "                 'content'  : subs,\n",
    "                 'duration' : duration, \n",
    "                 'level'    : level\n",
    "                }\n",
    "\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8dd9ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ‚úÖ**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "        <p>–°—É–ø–µ—Ä, —á—Ç–æ –æ—Ñ–æ—Ä–º–ª—è–µ—à—å –æ–±—Ä–∞–±–æ—Ç–∫—É  –≤ –≤–∏–¥–µ —Ñ—É–Ω–∫—Ü–∏–π! </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914eadf-60cf-4412-a009-bb87d29b3804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca37763",
   "metadata": {},
   "source": [
    "## –û–±—Ä–∞–±–æ—Ç–∫–∞ excel-—Ñ–∞–π–ª–∞\n",
    "\n",
    "–í –¥–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è —Å–ø–∏—Å–æ–∫ —Ñ–∏–ª—å–º–æ–≤ –∏ —É—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.\n",
    " \n",
    "–ó–∞–≥—Ä—É–∑–∏–º —Ñ–∞–π–ª –∏ –∏–∑—É—á–∏–º –µ–≥–æ:\n",
    "- —É–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "- –∏—Å–ø—Ä–∞–≤–∏–º –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π\n",
    "- –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π –æ—Å—Ç–∞–≤–∏–º –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "- —É–¥–∞–ª–∏–º —Ñ–∏–ª—å–º—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —É—Ä–æ–≤–Ω—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e329ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 241 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Movie   241 non-null    object\n",
      " 1   Level   241 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n",
      "\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 2\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ñ–∏–ª—å–º–æ–≤: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               movie   level\n",
       "id                                          \n",
       "0          10_Cloverfield_lane(2016)      B1\n",
       "1   10_things_I_hate_about_you(1999)      B1\n",
       "2               A_knights_tale(2001)      B2\n",
       "3               A_star_is_born(2018)      B2\n",
       "4                      Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_labels = pd.read_excel(f'{SCORES_PATH}/movies_labels.xlsx', index_col='id')\n",
    "movie_labels.info()\n",
    "movie_labels.columns = ['movie', 'level']\n",
    "print('\\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:', movie_labels.duplicated().sum())\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ñ–∏–ª—å–º–æ–≤:', movie_labels.movie.duplicated().sum())\n",
    "movie_labels = movie_labels.drop_duplicates()\n",
    "movie_labels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0885dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_50f82\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_50f82_level0_col0\" class=\"col_heading level0 col0\" >movie</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >level</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row0\" class=\"row_heading level0 row0\" >A2</th>\n",
       "      <td id=\"T_50f82_row0_col0\" class=\"data row0 col0\" >32 —Ñ–∏–ª—å–º–æ–≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row1\" class=\"row_heading level0 row1\" >B1</th>\n",
       "      <td id=\"T_50f82_row1_col0\" class=\"data row1 col0\" >58 —Ñ–∏–ª—å–º–æ–≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row2\" class=\"row_heading level0 row2\" >B2</th>\n",
       "      <td id=\"T_50f82_row2_col0\" class=\"data row2 col0\" >109 —Ñ–∏–ª—å–º–æ–≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f82_level0_row3\" class=\"row_heading level0 row3\" >C1</th>\n",
       "      <td id=\"T_50f82_row3_col0\" class=\"data row3 col0\" >40 —Ñ–∏–ª—å–º–æ–≤</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2ef150e8f0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct some mistakes in movies names\n",
    "movie_labels.movie = movie_labels.movie.str.replace('.srt', '', regex=False)\n",
    "movie_labels.loc[movie_labels.movie == 'Up (2009)', 'movie'] = 'Up(2009)'\n",
    "movie_labels.loc[movie_labels.movie == 'The Grinch', 'movie'] = 'The.Grinch'\n",
    "\n",
    "# —É–¥–∞–ª–∏–º –∏–∑ level –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –æ—Å—Ç–∞–≤–∏–º –ø—Ä–æ–±–µ–ª\n",
    "# –∏–∑ –º—É–ª—å—Ç–∏—É—Ä–æ–≤–Ω–µ–π –≤—ã–±–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª—å—à–∏–π\n",
    "movie_labels.level = movie_labels.level \\\n",
    "                                 .str.replace(',', '', regex=False) \\\n",
    "                                 .str.replace('+', '', regex=False) \\\n",
    "                                 .str.replace('/', ' ', regex=False) \\\n",
    "                                 .str.split().transform(lambda x: max(x))\n",
    "\n",
    "movie_labels.groupby('level').count().sort_index().style.format({'movie':'{:.0f} —Ñ–∏–ª—å–º–æ–≤'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b2f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Ñ–∏–ª—å–º–∞ The Secret Life of Pets.en\n",
      "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Ñ–∏–ª—å–º–∞ Glass Onion\n",
      "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Ñ–∏–ª—å–º–∞ Matilda(2022)\n",
      "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Ñ–∏–ª—å–º–∞ Bullet train\n",
      "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Ñ–∏–ª—å–º–∞ Thor: love and thunder\n",
      "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Ñ–∏–ª—å–º–∞ Lightyear\n"
     ]
    }
   ],
   "source": [
    "# excel processing\n",
    "for row in movie_labels.itertuples():\n",
    "\n",
    "    n = movies.loc[movies.filename.str.contains(row.movie, regex=False)].shape[0]\n",
    "\n",
    "    if n == 0:\n",
    "        print('–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Ñ–∏–ª—å–º–∞', row.movie)\n",
    "\n",
    "    elif n == 1:\n",
    "        selected_movie_level = movies.loc[\n",
    "            movies.filename.str.contains(row.movie, regex=False), 'level'].values[0]\n",
    "\n",
    "        if selected_movie_level == 'Subtitles':         # replace Subtitles with excel level\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'] = row.level\n",
    "\n",
    "        elif selected_movie_level != row.level:          # replace with max current level or excel\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'\n",
    "             ] = max(selected_movie_level, row.level)\n",
    "\n",
    "    else:\n",
    "        print('–¢–µ–∫—Å—Ç –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ', row.movie)\n",
    "\n",
    "\n",
    "movies = movies[movies.level!='Subtitles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db1473-a9a0-4c87-b946-cacbfb624886",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['target'] = movies.level.map({'A1':0, 'A2':1, 'B1':2, 'B2':3, 'C1':4, 'C2':5})\n",
    "movies = movies.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9664d-69d1-4701-b438-0c820a3d5775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2    144\n",
       "B1     55\n",
       "C1     39\n",
       "A2     32\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66906825-2a52-4133-96a1-142e799d9687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>duration</th>\n",
       "      <th>level</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crown, The S01E10 - Gloriana.en.FORCED</td>\n",
       "      <td>i am delighted to be here in cairo to meet wit...</td>\n",
       "      <td>[3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crown, The S01E04 - Act of God.en</td>\n",
       "      <td>fuel on. fuel on. chocks are in position. swit...</td>\n",
       "      <td>[2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...</td>\n",
       "      <td>good morning, connor. versace is on 1. okay. c...</td>\n",
       "      <td>[2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S01E06.1080p.BluRay.AAC5.1.x265-DTG.02.EN</td>\n",
       "      <td>harvey, i don't need a perp-walk or a front-pa...</td>\n",
       "      <td>[2.252, 1.208, 2.919, 3.044, 2.168, 2.668, 1.8...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suits.S02E04.HDTV.x264-ASAP</td>\n",
       "      <td>i want to, uh... taupe. is that...? justice th...</td>\n",
       "      <td>[2.001, 3.036, 1.634, 0.999, 1.032, 2.167, 1.7...</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Suits S04E09 EngSub</td>\n",
       "      <td>previously on suits... i want you to decide if...</td>\n",
       "      <td>[1.187, 4.012, 4.838, 1.878, 1.801, 3.386, 2.4...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Suits.S03E09.480p.HDTV.x264-mSD</td>\n",
       "      <td>previously on suits... i'm bonding with your f...</td>\n",
       "      <td>[1.454, 1.464, 1.692, 1.13, 1.3, 1.851, 2.786,...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Suits S04E10 EngSub</td>\n",
       "      <td>previously on suits... sheila amanda sazs, wil...</td>\n",
       "      <td>[1.205, 1.495, 1.177, 2.345, 2.478, 1.111, 2.7...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Downton Abbey - S01E03 - Episode 3.eng.SDH</td>\n",
       "      <td>there you are, mr. bates. it's in. came this m...</td>\n",
       "      <td>[2.708, 3.118, 1.832, 1.206, 2.287, 2.366, 1.7...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Suits S04E13 EngSub</td>\n",
       "      <td>and you got your name on the wall. now i want ...</td>\n",
       "      <td>[1.361, 1.641, 1.0, 1.143, 1.646, 1.444, 1.177...</td>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  \\\n",
       "0               Crown, The S01E10 - Gloriana.en.FORCED   \n",
       "1                    Crown, The S01E04 - Act of God.en   \n",
       "2    Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x2...   \n",
       "3      Suits.S01E06.1080p.BluRay.AAC5.1.x265-DTG.02.EN   \n",
       "4                          Suits.S02E04.HDTV.x264-ASAP   \n",
       "..                                                 ...   \n",
       "273                                Suits S04E09 EngSub   \n",
       "274                    Suits.S03E09.480p.HDTV.x264-mSD   \n",
       "275                                Suits S04E10 EngSub   \n",
       "276         Downton Abbey - S01E03 - Episode 3.eng.SDH   \n",
       "277                                Suits S04E13 EngSub   \n",
       "\n",
       "                                               content  \\\n",
       "0    i am delighted to be here in cairo to meet wit...   \n",
       "1    fuel on. fuel on. chocks are in position. swit...   \n",
       "2    good morning, connor. versace is on 1. okay. c...   \n",
       "3    harvey, i don't need a perp-walk or a front-pa...   \n",
       "4    i want to, uh... taupe. is that...? justice th...   \n",
       "..                                                 ...   \n",
       "273  previously on suits... i want you to decide if...   \n",
       "274  previously on suits... i'm bonding with your f...   \n",
       "275  previously on suits... sheila amanda sazs, wil...   \n",
       "276  there you are, mr. bates. it's in. came this m...   \n",
       "277  and you got your name on the wall. now i want ...   \n",
       "\n",
       "                                              duration level  target  \n",
       "0    [3.04, 3.0, 2.96, 2.6, 2.24, 1.48, 1.24, 2.72,...    B2       3  \n",
       "1    [2.12, 2.28, 1.84, 2.72, 5.6, 1.88, 2.8, 2.56,...    B2       3  \n",
       "2    [2.197, 1.433, 3.7, 2.299, 4.565, 3.335, 2.129...    B2       3  \n",
       "3    [2.252, 1.208, 2.919, 3.044, 2.168, 2.668, 1.8...    B2       3  \n",
       "4    [2.001, 3.036, 1.634, 0.999, 1.032, 2.167, 1.7...    B2       3  \n",
       "..                                                 ...   ...     ...  \n",
       "273  [1.187, 4.012, 4.838, 1.878, 1.801, 3.386, 2.4...    C1       4  \n",
       "274  [1.454, 1.464, 1.692, 1.13, 1.3, 1.851, 2.786,...    C1       4  \n",
       "275  [1.205, 1.495, 1.177, 2.345, 2.478, 1.111, 2.7...    C1       4  \n",
       "276  [2.708, 3.118, 1.832, 1.206, 2.287, 2.366, 1.7...    C1       4  \n",
       "277  [1.361, 1.641, 1.0, 1.143, 1.646, 1.444, 1.177...    C1       4  \n",
       "\n",
       "[270 rows x 5 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b829f3d-846f-44a8-8c61-14f7724a8c30",
   "metadata": {},
   "source": [
    "–û—á–µ–≤–∏–¥–Ω—ã–º —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏ –≤–æ–≤—Å–µ –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã - 'A1' –∏ 'C2'. –≠—Ç–æ –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–æ–∏—Ç –∏–º–µ—Ç—å –≤ –≤–∏–¥—É, –ø–æ–º–∏–º–æ –æ—á–µ–Ω—å —Å–∫—Ä–æ–º–Ω–æ–≥–æ –ø–æ –æ–±—ä–µ–º–∞–º –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf21de3-41af-42f3-a232-2188fd3184cc",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b67cb-b8b1-47b0-be9a-978628c1c727",
   "metadata": {},
   "source": [
    "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ Doc2Vec\n",
    "\n",
    "\n",
    "–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤ (–∞–Ω–≥–ª. word embedding) ‚Äî –æ–±—â–µ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏—é –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞–º –∏–∑ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ–±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. \n",
    "\n",
    "–î–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, –∞ –∑–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤. –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏: —Å–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤ —Ç–µ–∫—Å—Ç–µ —Ä—è–¥–æ–º —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (–∞ —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∏–º–µ—é—â–∏–µ —Å—Ö–æ–∂–∏–π —Å–º—ã—Å–ª), –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–æ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ. \n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –º–µ—à–∫–∞ —Å–ª–æ–≤, –º–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –Ω–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è +  —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–æ–≤–∞. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —ç–ø–æ—Ö - —á—Ç–æ–±—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–ª–æ–≤–∞ –¥–∞–ª–µ–∫–æ –Ω–µ —Ä–∞–∑–æ—à–ª–∏—Å—å - –∏–∑-–∑–∞ —ç—Ç–æ–≥–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∏—Å–∫ \"–ø–ª–æ—Ö–æ–≥–æ\" –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤ –¥–ª—è –Ω–∞—Å. –ï—Å–ª–∏ –±—ã –≤—ã–±–æ—Ä–∫–∞ –±—ã–ª–∞ –∑–Ω–∞—á–∏–º–æ –±–æ–ª—å—à–µ, —Ç–æ –º—ã —É–≤–µ–ª–∏—á–∏–ª–∏ –±—ã –∫–æ–ª–∏—á–µ—Ç–≤–æ —ç–ø–æ—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. \n",
    "–ü–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç—å —Å–ª–æ–≤–∞ —Å–æ —Å–ª–æ–≤–∞—Ä–µ–º Oxford. –ö –ø—Ä–∏–º–µ—Ä—É, –µ—Å—Ç—å —Å–ª–æ–≤–æ age –∫–∞–∫ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —É—Ä–æ–≤–Ω—é –∑–Ω–∞–Ω–∏–π A1. –ù–æ —Ç–æ –∂–µ —Å–ª–æ–≤–æ –∫–∞–∫ –≥–ª–∞–≥–æ–ª - —ç—Ç–æ —É–∂–µ –¥—Ä—É–≥–æ–π —É—Ä–æ–≤–µ–Ω—å, –≤—ã—à–µ. \n",
    "\n",
    "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –¥–∞–Ω–Ω—ã–π —Ñ–∞–∫—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –° –¥–∞–Ω–Ω–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é —è–∑—ã–∫–∞ –æ—Ç–ª–∏—á–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å. –°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∫–ª–∞—Å—Å—ã –¥–∏—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã + –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –≤ —Ç—Ä–∞–∫—Ç–æ–≤–∫–∞—Ö \"–∫–ª–∞—Å—Å–æ–≤\" - –∫–æ–≥–¥–∞ A2/A2+, B1. –í—Å–ø–æ–º–Ω–∏–º \"–∑–æ–ª–æ—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\" - ¬†garbage in - garbage out. –¢.–µ. –¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –≤ –±—É–¥—É—â–µ–º –Ω–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π (—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–æ–π)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417679f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ‚úÖ**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "        <p>–û—Ç–ª–∏—á–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15baef26-d7cd-4167-8a58-f2f1e2c8cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove stop words from word list\n",
    "def remove_stop_words(sample_words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sample_words = [x for x in sample_words if not x in stop_words]\n",
    "    return sample_words\n",
    "\n",
    "##remove special characters from word list\n",
    "def remove_special_char(sample_words):\n",
    "    special_char = set(punctuation) \n",
    "    sample_words = [x for x in sample_words if not x in special_char]    \n",
    "    return sample_words\n",
    "\n",
    "##lemmatize words in word list\n",
    "def lemmatizer(sample_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sample_words = [lemmatizer.lemmatize(x) for x in sample_words]\n",
    "    return sample_words\n",
    "\n",
    "##all words in lower case\n",
    "def lower_case(sample_words):\n",
    "    sample_words = [x.lower() for x in sample_words]\n",
    "    return sample_words\n",
    "\n",
    "##normalize a word list (if document already tokenized)\n",
    "def normalize_word_list(sample_words,\n",
    "                        lowercase=True,\n",
    "                        stopwords=True,\n",
    "                        specialchar=True,\n",
    "                        lemmatize=True):\n",
    "    if lowercase:\n",
    "        sample_words = lower_case(sample_words)\n",
    "    if stopwords:\n",
    "        sample_words = remove_stop_words(sample_words)\n",
    "    if specialchar:\n",
    "        sample_words = remove_special_char(sample_words)\n",
    "    if lemmatize:\n",
    "        sample_words = lemmatizer(sample_words)\n",
    "    sample_words = ' '.join(sample_words)\n",
    "    return sample_words\n",
    "\n",
    "##normalize a list of sentences\n",
    "def normalize_sent_list(sample_sents,\n",
    "                        lowercase=True,\n",
    "                        stopwords=True,\n",
    "                        specialchar=True,\n",
    "                        lemmatize=True):    \n",
    "    print(\"Pre-processing text ...\")\n",
    "    sent_list = sample_sents\n",
    "    for i in range(len(sample_sents)):\n",
    "        sent_list[i] = re.findall(r\"[\\w']+|[.,!?;]\", sent_list[i])\n",
    "        sent_list[i] = normalize_word_list(sent_list[i],\n",
    "                            lowercase=True,\n",
    "                            stopwords=True,\n",
    "                            specialchar=True,\n",
    "                            lemmatize=True)\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71a2a5-f8fd-4f89-8e4c-bb3c4e4b5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data_file):\n",
    "    ## read training data as a pandas dataframe\n",
    "    orig_df = data_file\n",
    "    ## text preprocessing\n",
    "    text = pd.Series.tolist((orig_df['content']))\n",
    "    x_target = pd.Series.tolist((orig_df['target']))\n",
    "    x_level = pd.Series.tolist((orig_df['level']))\n",
    "    \n",
    "    text = normalize_sent_list(text,\n",
    "                        \t\tlowercase=True,\n",
    "                        \t\tstopwords=True,\n",
    "                        \t\tspecialchar=True,\n",
    "                        \t\tlemmatize=False)\n",
    "    ## preparing preprocessed text\n",
    "    text_df = pd.DataFrame(text, columns=[\"content\"])\n",
    "    text_df['level'] = x_level\n",
    "    text_df['target'] = x_target\n",
    "    print(text_df)\n",
    "    return text_df\n",
    "\n",
    "def Doc2VecModel(text_df, no_epochs, val_split_ratio):\n",
    "    ## splitting dataframe into training and validation frames\n",
    "    train_df, val_df = train_test_split(text_df, test_size=val_split_ratio, random_state=RANDOM_STATE)\t\n",
    "    ## creating tagged documents\n",
    "    train_tagged = train_df.apply(\n",
    "        lambda r: TaggedDocument(words=r['content'].split(), tags=str(r.target)), axis=1)\n",
    "    val_tagged = val_df.apply(\n",
    "        lambda r: TaggedDocument(words=r['content'].split(), tags=str(r.target)), axis=1)\n",
    "    ## building a distributed bag of words model \n",
    "    cores = multiprocessing.cpu_count()\n",
    "    print(\"Building the Doc2Vec model vocab...\")\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=2000, negative=5, min_count=3, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    ## training the model\n",
    "    print(\"Training the Doc2Vec model for\", no_epochs, \"number of epochs\" )\n",
    "    for epoch in range(no_epochs):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), \n",
    "                total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    ## preparing document vectors for learning\n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_val, X_val = vec_for_learning(model_dbow, val_tagged)\n",
    "    ## training RandomForestClassifier model\n",
    "    print(\"Training RandomForestClassifier model...\")\n",
    "    rfc=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3],\n",
    "    'min_samples_split': [5, 8, 10],\n",
    "    'n_estimators': [100, 150],\n",
    "    'criterion': ['entropy']\n",
    "    }\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, scoring = 'f1', cv=5)\n",
    "    CV_rfc.fit(X_train, y_train)\n",
    "    print(CV_rfc.best_params_)\n",
    "    b_rfc = CV_rfc.best_estimator_\n",
    "    ## making predictions on the training set\n",
    "    print(\"Prediction numbers:\")\n",
    "    train_binary = b_rfc.predict(X_train)\n",
    "    print('Accuracy on the training set : %s' % accuracy_score(y_train, train_binary))\n",
    "    print('F1 score on the training set : {}'.format(f1_score(y_train, train_binary, average='weighted')))\n",
    "    ## making predictions on the validation set\n",
    "    val_binary = b_rfc.predict(X_val)\n",
    "    print('Accuracy on the validation set : %s' % accuracy_score(y_val, val_binary))\n",
    "    print('F1 score on the validation set : {}'.format(f1_score(y_val, val_binary, average='weighted')))\n",
    "    return model_dbow, b_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80597912",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ‚úÖ**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞** </b>\n",
    "    <p> –ü—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä! –¢—ã –¥–∞–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π! –≠—Ç–æ –∫—Ä—É—Ç–æ! </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aca8101c-bacd-4c44-80e4-d1cb1d2331b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text ...\n",
      "                                               content level  target\n",
      "0    delighted cairo meet colonel nasser continue d...    B2       3\n",
      "1    fuel fuel chock position switch sure sir got m...    B2       3\n",
      "2    good morning connor versace 1 okay clear good ...    B2       3\n",
      "3    harvey need perp walk front page headline want...    B2       3\n",
      "4    want uh taupe justice thomas knew louis ah poi...    B2       3\n",
      "..                                                 ...   ...     ...\n",
      "265  previously suit want decide love hate want com...    C1       4\n",
      "266  previously suit i'm bonding father speaking ta...    C1       4\n",
      "267  previously suit sheila amanda sazs marry yes i...    C1       4\n",
      "268  mr bates came morning said would quite thing h...    C1       4\n",
      "269  got name wall want respect come meaning want o...    C1       4\n",
      "\n",
      "[270 rows x 3 columns]\n",
      "Building the Doc2Vec model vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [00:00<00:00, 553280.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Doc2Vec model for 3 number of epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [00:00<00:00, 2325655.24it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [00:00<00:00, 2395250.91it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [00:00<00:00, 888525.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestClassifier model...\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Prediction numbers:\n",
      "Accuracy on the training set : 1.0\n",
      "F1 score on the training set : 1.0\n",
      "Accuracy on the validation set : 0.6585365853658537\n",
      "F1 score on the validation set : 0.5982632967284168\n"
     ]
    }
   ],
   "source": [
    "no_epochs = 3\n",
    "\n",
    "val_split_ratio = 0.15\n",
    "\n",
    "## preparing training data\n",
    "text_df = prepare_training_data(movies)\n",
    "\n",
    "## building the document vector model\n",
    "model_dbow, classifier = Doc2VecModel(text_df, no_epochs,  val_split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8609b-9b84-4589-bb63-5196bb931b62",
   "metadata": {},
   "source": [
    "# –ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228dd6c8-cb09-4c93-9dcf-c427f1021b13",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –∏—Ç–æ–≥, –º—ã –ø–æ–ª—É—á–∏–ª–∏ –≤ —Ä–∞–º–∫–∞—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–µ–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. \n",
    "–¢–∞–∫, –º—ã –ø–æ–ª—É—á–∏–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç F1 score –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 60%, —á—Ç–æ –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å –±–æ–ª–µ–µ —á–µ–º —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∏ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Å–æ —Å–ø–æ—Ä–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5855d",
   "metadata": {},
   "source": [
    "<div style=\"border:solid Green 2px; padding: 40px\">\n",
    "\n",
    "<b>**–û–±—â–∏–π –≤—ã–≤–æ–¥ –ø–æ –ø—Ä–æ–µ–∫—Ç—É**</b>\n",
    "\n",
    "–°–ø–∞—Å–∏–±–æ –∑–∞ —Ç–≤–æ–π –ø—Ä–æ–µ–∫—Ç! –í–∏–¥–Ω–æ, —á—Ç–æ –≤ –Ω–µ–≥–æ –≤–ª–æ–∂–µ–Ω–æ –º–Ω–æ–≥–æ —Å–∏–ª –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –±–æ–ª—å—à–∞—è —Ä–∞–±–æ—Ç–∞. –°—É–±—Ç–∏—Ç—Ä—ã –æ—Ç–ª–∏—á–Ω–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, —Å–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å, –µ—Å—Ç—å –ø–µ—Ä–µ–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –°—É–ø–µ—Ä!\n",
    "<b>–ù–∞ —á—Ç–æ —Å—Ç–æ–∏—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞:</b>\n",
    "   \n",
    "* –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">TfIdf-–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏</a>. –û–¥–Ω–æ –∏–∑ –µ–≥–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω —É—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤–∞ –≤–æ –≤—Å—ë–º –¥–∞—Ç–∞—Å–µ—Ç–µ, –∏, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –¥–µ–ª–∞–µ—Ç —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏. –í —Å–ª—É—á–∞–µ —Å —Ä–µ—à–µ–Ω–∏–µ–º –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É—Ä–æ–≤–Ω–µ–π —è–∑—ã–∫–∞ —ç—Ç–æ –ø–æ–ª–µ–∑–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, —Ç–∞–∫ –∫–∞–∫ –∏ –≤ —Å–∞–º–æ–º —è–∑—ã–∫–µ —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ –≤–æ –º–Ω–æ–≥–æ–º –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —É—Ä–æ–≤–µ–Ω—å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å–ª—É—à–∞—é—â–µ–≥–æ/–≥–æ–≤–æ—Ä—è—â–µ–≥–æ.\n",
    "* –ï—Å–ª–∏ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å, —Ç–æ –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ –±—É–¥–µ—Ç –∏–∑—É—á–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É <a href=\"https://optuna.org/\">optuna</a> –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å –Ω–µ–π –ø–µ—Ä–µ–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Ä–∞–∑—ã –ø—Ä–æ—â–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ!\n",
    "* –¢–∞–∫–∂–µ, –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–æ–∑–¥–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, —Ç–µ–º —Å–∞–º—ã–º —Ä–∞—Å—à–∏—Ä–∏–≤ –Ω–∞—à–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "\n",
    "<b>**–ñ–µ–ª–∞—é —É–¥–∞—á–∏!**</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e92edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
